Building DAG of jobs...
Using shell: /home/giacomocastagnetti/.nextstrain/runtimes/conda/env/bin/bash
Provided cores: 1 (use --cores to define parallelism)
Rules claiming more threads will be scaled down.
Job stats:
job          count    min threads    max threads
---------  -------  -------------  -------------
filtering        1              1              1
total            1              1              1

Select jobs to execute...

[Tue Jul 18 14:39:41 2023]
rule filtering:
    input: data/sequences.fasta, results/index.tsv, data/metadata.csv
    output: results/filtered_sequences.fa, results/filtered_metadata.csv
    jobid: 0
    reason: Missing output files: results/filtered_metadata.csv, results/filtered_sequences.fa
    resources: tmpdir=/tmp

[Tue Jul 18 14:39:42 2023]
Error in rule filtering:
    jobid: 0
    input: data/sequences.fasta, results/index.tsv, data/metadata.csv
    output: results/filtered_sequences.fa, results/filtered_metadata.csv
    shell:
        
        augur filter             --sequences data/sequences.fasta            --sequence-index results/index.tsv            --metadata data/metadata.csv            --metadata-id-columns 'Accession'            --subsample-max-sequences 5000            --probabilistic-sampling            --output results/filtered_sequences.fa            --output-metadata results/filtered_metadata.csv
        
        (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)

Removing output files of failed job filtering since they might be corrupted:
results/filtered_sequences.fa, results/filtered_metadata.csv
Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
Complete log: .snakemake/log/2023-07-18T143941.759727.snakemake.log
